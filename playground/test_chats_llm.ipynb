{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "openagent_dir = os.path.dirname(os.path.abspath(\"\"))\n",
    "sys.path.append(openagent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# from openagent.tools.wrappers.serpapi import SerpAPIWrapper\n",
    "from openagent import compiler\n",
    "from openagent.llms._openai import OpenAI\n",
    "\n",
    "# load .env file in format:\n",
    "# OPENAI_API_KEY=sk-xxx\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llmcc = OpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "llmtc = OpenAI(\n",
    "    model=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to use directly llm from guidance, without prompt_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this session, only require a formatted prompt from prompt engine and return a completion\n",
    "# llmcc_session = llmcc.session()  \n",
    "# llmcc_session(\"Hello\") # doesn't work\n",
    "\n",
    "\n",
    "# # it still doesn't work\n",
    "# llmcc_session('''{{#user~}}\n",
    "#                 Tell me about Delhi\n",
    "#                 {{~/user}}\n",
    "                                            \n",
    "#                 {{#assistant~}}\n",
    "#                 {{gen 'info_about_delhi' temperature=0 max_tokens=300}}\n",
    "#                 {{~/assistant}}''')\n",
    "\n",
    "\n",
    "# # this is how it should work (prompt should have \"<|im_start|>assistant\\n\" at the end before gen tag)\n",
    "# # to check the tags like <|im_start|>assistant\\n, print output or check prompt.text property\n",
    "# llmcc_session('''{{#user~}}\n",
    "#                 Tell me about Delhi\n",
    "#                 {{~/user}}\n",
    "                                            \n",
    "#                 {{#assistant~}}\n",
    "#                 ''')\n",
    "\n",
    "\n",
    "# # this works well \n",
    "# llmtc_session = llmtc.session()\n",
    "# llmtc_session(prompt=\"Hello, my name is Karan, here is my resume - \", caching = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case to pass only text in assistant tag (to check if it follows the instruction) # works\n",
    "\n",
    "prompt = compiler('''{{#user~}}\n",
    "                            Tell me about {{'city'}}\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            Yes, I will tell you about Delhi in Hindi\n",
    "                            {{~/assistant}}\n",
    "                       \n",
    "                            {{#user~}}\n",
    "                            Yes, tell me\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            {{gen 'info_about_delhi' temperature=0 max_tokens=300}}\n",
    "                            {{~/assistant}}''', llm = llmcc)\n",
    "\n",
    "# prompt = prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"Compiler-stop-button-aeb168f2-527f-4b04-8a29-90fa7efd9a9f\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"Compiler-content-aeb168f2-527f-4b04-8a29-90fa7efd9a9f\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#user~}}</span>\n",
       "                            Tell me about Delhi\n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/user}}</span>\n",
       "                                                        \n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#assistant~}}</span>\n",
       "                            Yes, I will tell you about Delhi in Hindi\n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/assistant}}</span>\n",
       "                       \n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#user~}}</span>\n",
       "                            Yes, tell me\n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/user}}</span>\n",
       "                                                        \n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#assistant~}}</span>\n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &#x27;info_about_delhi&#x27; temperature=0 max_tokens=300}}</span>\n",
       "                       \n",
       "                            also here is the info about nearby cities \n",
       "                       \n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &#x27;info_about_delhi_nearby_cities&#x27; temperature=0 max_tokens=300}}</span>\n",
       "                            <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/assistant}}</span></pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"Compiler_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"Compiler-content-\"+t),this.stop_button=document.getElementById(\"Compiler-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._CompilerDisplay=function(t,e){return new d(t,e)}})()})();; window._CompilerDisplay(\"aeb168f2-527f-4b04-8a29-90fa7efd9a9f\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test case to pass both text and gen tag (doesn't work because gen tags not supported after text)\n",
    "\n",
    "prompt = compiler('''{{#user~}}\n",
    "                            Tell me about Delhi\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            Yes, I will tell you about Delhi in Hindi\n",
    "                            {{~/assistant}}\n",
    "                       \n",
    "                            {{#user~}}\n",
    "                            Yes, tell me\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            {{gen 'info_about_delhi' temperature=0 max_tokens=300}}\n",
    "                       \n",
    "                            also here is the info about nearby cities \n",
    "                       \n",
    "                            {{gen 'info_about_delhi_nearby_cities' temperature=0 max_tokens=300}}\n",
    "                            {{~/assistant}}''', llm = llmcc)\n",
    "\n",
    "prompt = prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case to pass both text and gen tag in assistant (properly!) to guide llm for further outputs (works). Only text after gen tag works, not before\n",
    "\n",
    "prompt = compiler('''{{#user~}}\n",
    "                            Tell me about Delhi\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            Yes, I will tell you about Delhi in Hindi\n",
    "                            {{~/assistant}}\n",
    "                       \n",
    "                            {{#user~}}\n",
    "                            Yes, tell me\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            {{gen 'info_about_delhi' temperature=0 max_tokens=300}}\n",
    "                            Let me also tell about some nearby cities to Delhi:\n",
    "                            {{~/assistant}}\n",
    "\n",
    "                            {{#assistant~}}\n",
    "                             {{gen 'info_about_nearby_cities' temperature=0 max_tokens=300}}\n",
    "                              let me also tell you about Jaipur\n",
    "                            {{~/assistant}}\n",
    "                       \n",
    "                              {{#assistant~}}\n",
    "                             {{gen 'info_about_jaipur' temperature=0 max_tokens=300}}\n",
    "                              \n",
    "                            {{~/assistant}}\n",
    "                       \n",
    "                       ''', llm = llmcc)\n",
    "\n",
    "prompt = prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case to check output formatting using text completion llm\n",
    "\n",
    "prompt = compiler(\n",
    "'''Here is something you should know about {{actor}}\n",
    "He was a short man, may be {{gen 'height' max_tokens=2 top_p =1.0}} height, and had {{gen 'hair_color' max_tokens=2 top_p = 1.0}} hair.\n",
    "He was known for his {{gen 'quality1' max_tokens= 1}} and {{gen 'quality2' max_tokens=1}}\n",
    "''', llm = llmtc, actor = \"\")\n",
    "\n",
    "# print(str(prompt))\n",
    "\n",
    "prompt.variables()\n",
    "\n",
    "# prompt._variables\n",
    "# prompt = prompt(actor = \"Tyrion Lannister\")\n",
    "# prompt = prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case to check dynamic flow tests: works\n",
    "\n",
    "dynamic_flow = compiler(\n",
    "                        \"\"\"{{#user~}}\n",
    "                            Tell me about Delhi. Cover history, famous places, food, culture, and majorly IT industry.\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            {{gen 'info_about_delhi' temperature=0 max_tokens=300}}\n",
    "                            {{~/assistant}}\n",
    "\n",
    "                            {{#user~}}\n",
    "                            Tell me about Bengaluru. Cover history, famous places, food, culture, and majorly IT industry.\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            {{gen 'info_about_bengaluru' temperature=1 max_tokens=200}}\n",
    "                            {{~/assistant}}\n",
    "                                                        \n",
    "                            {{#user~}}\n",
    "                            Consider {{info_about_delhi}} AND {{info_about_bengaluru}} and tell me which city is better than other in terms of Startups and Founders\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            {{gen 'response' temperature=1 max_tokens=400}}\n",
    "                            {{~/assistant}}\n",
    "\n",
    "                            {{#user~}}\n",
    "                            Which city has more population?\n",
    "                            {{~/user}}\n",
    "                                                        \n",
    "                            {{#assistant~}}\n",
    "                            {{gen 'response' temperature=1 max_tokens=400}}\n",
    "                            {{~/assistant}}                                                                 \n",
    "                    \"\"\"\n",
    "                    )\n",
    "\n",
    "prompt = dynamic_flow()\n",
    "# working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case to check chat loop: works\n",
    "\n",
    "chat_loop1 = compiler(\n",
    "                '''{{#system}}You are a helpful assistant{{/system}}\n",
    "\n",
    "                    {{~#geneach 'conversation' stop=False}}\n",
    "\n",
    "                    {{#user~}}\n",
    "                    {{set 'this.user_text' (await 'user_text') hidden=False}}\n",
    "                    {{~/user}}\n",
    "\n",
    "                    {{#assistant~}}\n",
    "                    {{gen 'this.response' temperature=0 max_tokens=300}}\n",
    "                    {{~/assistant}}\n",
    "                    \n",
    "                    {{~/geneach}}''') \n",
    "\n",
    "\n",
    "# manual conversation (in manual conversation, next input must be provide to return value)\n",
    "chat_loop2 = chat_loop1(user_text = \"My name is yash\") # chat_loop2 has the conversation of chat_loop1\n",
    "chat_loop3 = chat_loop2(user_text = \"What is my name?\") # chat_loop3 has the conversation of chat_loop2\n",
    "\n",
    "\n",
    "# chat loop (in a loop, both function (chat_loop1) and returned value (chat_loop2) should have same name)\n",
    "n = 0\n",
    "while n < 5:\n",
    "    chat_loop = chat_loop(user_text = input())\n",
    "    print(chat_loop['conversation'])\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot demonstration\n",
    "\n",
    "def chatbot(input, chat_loop):\n",
    "\n",
    "    if chat_loop is None:\n",
    "        chat_loop = compiler(\n",
    "                    '''{{#system}}You are a helpful assistant{{/system}}\n",
    "\n",
    "                        {{~#geneach 'conversation' stop=False}}\n",
    "\n",
    "                        {{#user~}}\n",
    "                        {{set 'this.user_text' (await 'user_text') hidden=False}}\n",
    "                        {{~/user}}\n",
    "\n",
    "                        {{#assistant~}}\n",
    "                        {{gen 'this.response' temperature=0 max_tokens=300}}\n",
    "                        {{~/assistant}}\n",
    "                        \n",
    "                        {{~/geneach}}''', llm = llmcc, silent = True) \n",
    "    \n",
    "    chat_loop = chat_loop(user_text = input) \n",
    "    return chat_loop['conversation'][-2]['response'], chat_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp, loop = chatbot(\"Hello\", None)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp, loop = chatbot(\"How do we train a ML model? Explain in 50 words only\", loop)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
